import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import os

def get_first_paper_link(driver, title):
    # Cochrane Library 검색 페이지로 이동
    search_url = "https://www.cochranelibrary.com/search"
    driver.get(search_url)

    # 검색창 찾기 및 논문 제목 입력
    search_box = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.ID, "searchText"))
    )
    search_box.clear()
    search_box.send_keys(title)
    search_box.send_keys("\n")

    # 첫 번째 검색 결과의 링크 추출
    first_result = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CSS_SELECTOR, ".search-results-item .result-title a"))
    )
    paper_link = first_result.get_attribute('href')
    
    return paper_link

def extract_pico(url):
    # Selenium 설정
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # GUI 없이 실행
    service = Service('/Users/myo/development/chromedriver')  # chromedriver 경로를 지정해주세요
    driver = webdriver.Chrome(service=service, options=chrome_options)

    driver.get(url)
    
    # PICO 섹션이 로드될 때까지 대기
    wait = WebDriverWait(driver, 10)
    wait.until(EC.presence_of_element_located((By.ID, "pico")))
    
    # 페이지 스크롤
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(2)  # 스크롤 후 콘텐츠 로드를 위한 대기

    # 페이지 소스 가져오기
    page_source = driver.page_source
    soup = BeautifulSoup(page_source, 'html.parser')

    pico = {}
    pico_section = soup.find('section', class_='pico-section', id='pico')
    
    if pico_section:
        for column in pico_section.find_all('div', class_='pico-column'):
            pico_type = column['class'][1]  # Population, Intervention, Comparison, Outcome
            terms = [a.text.strip() for a in column.find_all('a')]
            pico[pico_type] = terms

    driver.quit()
    return pico

def process_papers(start_index, end_index, titles_df, output_file):
    # Selenium 설정
    chrome_options = Options()
    service = Service('/Users/myo/development/chromedriver')  # chromedriver 경로를 지정해주세요
    driver = webdriver.Chrome(service=service, options=chrome_options)

    pico_data = []

    # 지정된 범위의 논문에 대해 링크 추출 및 PICO 정보 추출
    for index in range(start_index, end_index):
        if index >= len(titles_df):
            break
        
        row = titles_df.iloc[index]
        title = row['Title']
        year_issue = f"{row['Year']}-{row['Issue']}"
        print(f"Processing {index+1}/{len(titles_df)}: {title}")

        try:
            # 논문의 첫 번째 링크 추출
            paper_link = get_first_paper_link(driver, title)
            print(f"Found paper link: {paper_link}")

            # PICO 정보 추출
            pico_info = extract_pico(paper_link)
            pico_info["No."] = index  # 인덱스가 1부터 시작하므로 그대로 사용
            pico_info["Year-Issue"] = year_issue
            pico_info["Title"] = title  # PICO 정보에 제목 추가
            pico_data.append(pico_info)
            print(f"PICO data for {title}: {pico_info}")

        except Exception as e:
            print(f"Error processing title: {title}, error: {e}")
            # 오류 발생 시, 제목만 저장하고 나머지는 공백으로 처리
            pico_data.append({"No.": index, "Year-Issue": year_issue, "Title": title, "Population": "", "Intervention": "", "Comparison": "", "Outcome": ""})

    driver.quit()

    # 결과를 DataFrame으로 변환
    df = pd.DataFrame(pico_data, columns=["No.", "Year-Issue", "Title", "Population", "Intervention", "Comparison", "Outcome"])

    # 기존 CSV 파일에 추가
    if os.path.exists(output_file):
        # 파일의 끝에 데이터를 추가
        with open(output_file, 'a') as f:
            df.to_csv(f, header=False, index=False)
    else:
        # 파일이 없으면 새로 작성
        df.to_csv(output_file, index=False)

    print(f"Saved PICO data to {output_file}")

def main():
    # CSV 파일에서 논문 제목 읽기
    input_file = '/Users/myo/Desktop/Kangs/interv_sr_list.csv'
    titles_df = pd.read_csv(input_file)

    # 출력 파일 설정
    output_file = '/Users/myo/Desktop/Kangs/picos_good.csv'

    # 이미 처리된 마지막 논문 번호 확인
    if os.path.exists(output_file):
        existing_data = pd.read_csv(output_file)
        last_processed_no = int(existing_data['No.'].max())
    else:
        last_processed_no = 0

    # 다음 논문부터 처리 시작 (데이터는 0부터 시작하지만 실제 논문 번호는 1번부터 시작)
    start_index = last_processed_no + 1  # 기존에 처리한 논문 바로 다음부터 시작
    for start_index in range(start_index, len(titles_df), 100):
        end_index = start_index + 100
        process_papers(start_index, end_index, titles_df, output_file)

if __name__ == "__main__":
    main()
